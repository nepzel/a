mkdir wordcount
----------------
cd wordcount
------------
touch input.txt
-----------
vi input.txt
****put shit inside****
hai
ga ga gu ga gay man u
fuk in ohio happening
shit this big daddy has bigbonkerz
dad
u
went
for
milk
,but
u 
didn't
return
back
ga ga gu ga gay man u
fuk in ohio happening
shit this big daddy has bigbonkerz
dad
u
went
for
milk
,but
u 
didn't
return
back
----------------------
create a word count java file:

touch wordcount.java
vi wordcount.java
****put this shit***
public void reduce(Text key, Iterable<IntWritable>values,Context context)
throws IOException,InterruptedException{
int sum=0;
for(IntWritable x:values)
{
sum+=x.get();
}
context.write(key,new IntWritable(sum));
}
}
public static void main(String[]args) throws Exception{
Configuration conf=new Configuration();
Job job=new Job(conf,"My word count program");
job.setJarByClass(wordcount.class);
--INSERT--
----------------
After above step put this code^^

SOURCE CODE:
import java.io.IOException;
import java.util.StringTokenizer;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.fs.Path;

public class WordCount
{
public static class Map extends Mapper<LongWritable,Text,Text,IntWritable> {
public void map(LongWritable key, Text value,Context context) throws

IOException,InterruptedException{
String line = value.toString();
StringTokenizer tokenizer = new StringTokenizer(line);
while (tokenizer.hasMoreTokens()) {
value.set(tokenizer.nextToken());
context.write(value, new IntWritable(1));
}
}
}
public static class Reduce extends Reducer<Text,IntWritable,Text,IntWritable> {
public void reduce(Text key, Iterable<IntWritable> values,Context context)
throws IOException,InterruptedException {
int sum=0;
for(IntWritable x: values)
{
sum+=x.get();
}
context.write(key, new IntWritable(sum));
}
}

public static void main(String[] args) throws Exception {

Configuration conf= new Configuration();
Job job = new Job(conf,"My Word Count Program");
job.setJarByClass(WordCount.class);
job.setMapperClass(Map.class);
job.setReducerClass(Reduce.class);
job.setOutputKeyClass(Text.class);
job.setOutputValueClass(IntWritable.class);
job.setInputFormatClass(TextInputFormat.class);
job.setOutputFormatClass(TextOutputFormat.class);
Path outputPath = new Path(args[1]);
//Configuring the input/output path from the filesystem into the job
FileInputFormat.addInputPath(job, new Path(args[0]));

FileOutputFormat.setOutputPath(job, new Path(args[1]));
//deleting the output path automatically from hdfs so that we don't have to delete it explicitly
outputPath.getFileSystem(conf).delete(outputPath);
//exiting the job only if the flag value becomes false
System.exit(job.waitForCompletion(true) ? 0 : 1);
}
}
--------------------------------------------------------------------------------------------------------
CREATE A DIRECTORY/ FOLDER CLASSES INSIDE THE WORDCOUNT FOLDER:
mkdir classes
----------------------------------------------------------------------
SET THE PATH FOR JAVA FILE:
COMMAND: 
export HADOOP_CLASSPATH=$(hadoop classpath)
echo $HADOOP_CLASSPATH
-------------------------------------------
Compile the java File:
Command:
javac -classpath ${HADOOP_CLASSPATH} -d '/home/kct5thsemcdhi061/wordcount/classes''/home/kct5thsemcdhi061/wordcount/wordcount.java'
-----------------------------------------------------
CREATE A JAR FILE:
COMMAND: jar -cvf WordCount.jar -C ‘/home/<NAME>/wordcount/classes’/ .
^^^that  last dot is important^^^^^^^^^^^
-----------------------------------------------------------------------
CREATE A DIRECTORY IN HADOOP:
hdfs dfs -mkdir -p /user/kct5thsemcdhi061/wordcount
hdfs dfs -mkdir -p /user/kct5thsemcdhi061/wordcount/input/
-----------------------------------------------------------------------
PUT THE INPUT FILE IN LOCAL SYSTEM TO HADOOP DIRECTORY:
hdfs dfs -put /home/kct5thsemcdhi061/wordcount/input/
-------------------------------------------------------------------
CREATE A OUTPUT DIRECTORY IN HDFS INSIDE THE WORDCOUNT:
hdfs dfs mkdir /user/kct5thsemcdhi061/wordcount/output/
---------------------------------------------------------------
RUN THE MAP REDUCE PROGRAM:
COMMAND: 
hadoop jar /home/kct5thsemcdhi061/wordcount/wordcount.jar wordcount/user/kct5thsemcdhi061/wordcount/input/user/kct5thsemcdhi061/wordcount/output
-------------------------------------------------------------------------------
VERIFY THE OUTPUT:
hdfs dfs -cat /user/kct5thsemcdhi061/wordcount/output/*
______________________________________________________________________xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx_____________________________________________________
